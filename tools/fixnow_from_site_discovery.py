"""
Fix-Now SEO Analysis from Site Discovery Data
Uses the audit_report.csv from site-discovery tool
"""
import os, re, csv, json, datetime, urllib.parse
from collections import defaultdict, Counter

# Paths
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
AUDIT_CSV = os.path.join(PROJECT_ROOT, "site-discovery", "output", "audit_report.csv")
GSC_DIR = os.path.join(PROJECT_ROOT, "gsc_data")
OUTB = os.path.join(PROJECT_ROOT, "website", "seo-briefs")
OUTR = os.path.join(PROJECT_ROOT, "website", "seo-remediation")

os.makedirs(OUTB, exist_ok=True)
os.makedirs(os.path.join(OUTR, "fixes"), exist_ok=True)

print(f"ğŸ“Š Analyzing site-discovery audit data...")
print(f"Source: {AUDIT_CSV}\n")

def cap(s, n):
    s = re.sub(r"\s+", " ", str(s)).strip()
    return (s[:n-1]+"â€¦") if len(s) > n else s

def https(u):
    if not isinstance(u, str):
        return u
    if u.startswith("https://"):
        return u
    if u.startswith("http://"):
        return "https://" + u[7:]
    return u

def tokens(url):
    path = urllib.parse.urlparse(url).path.lower()
    return [t for t in re.split(r"[^a-z0-9]+", path) if t and t not in {'en', 'us', 'gb', 'de', 'fr', 'es', 'ko', 'kr', 'www', 'ai', 'com', 'org', 'net'}]

# Read audit report
rows = []
with open(AUDIT_CSV, 'r', encoding='utf-8-sig') as f:
    reader = csv.DictReader(f)
    for row in reader:
        rows.append(row)

print(f"âœ“ Loaded {len(rows)} pages from audit")

# Analyze meta tags
issues = {
    'empty_titles': [],
    'empty_descriptions': [],
    'duplicate_titles': [],
    'duplicate_descriptions': [],
    'http_canonicals': [],
    'missing_canonicals': [],
    'duplicate_faqpage': []
}

title_counts = Counter()
desc_counts = Counter()
netloc = None

for row in rows:
    url = row.get('url', '') or row.get('URL', '')
    if not url:
        continue
    
    if not netloc:
        netloc = urllib.parse.urlparse(url).netloc
    
    status = row.get('status', '') or row.get('Status', '')
    if status and str(status).startswith('2'):  # Only 2xx responses
        title = (row.get('title', '') or row.get('Title', '')).strip()
        meta_desc = (row.get('meta_description', '') or row.get('MetaDescription', '')).strip()
        canonical = (row.get('canonical', '') or row.get('Canonical', '')).strip()
        jsonld = row.get('jsonld_types', '') or row.get('JSONLD_Types', '') or ''
        
        # Empty checks
        if not title or title == 'nan':
            issues['empty_titles'].append({'url': url, 'title': title})
        else:
            title_counts[title] += 1
        
        if not meta_desc or meta_desc == 'nan':
            issues['empty_descriptions'].append({'url': url, 'description': meta_desc})
        else:
            desc_counts[meta_desc] += 1
        
        # Canonical checks
        if not canonical or canonical == 'nan':
            issues['missing_canonicals'].append({'url': url})
        elif canonical.startswith('http://'):
            issues['http_canonicals'].append({'url': url, 'canonical': canonical})
        
        # Schema checks
        if 'FAQPage' in jsonld:
            # Count occurrences
            faq_count = jsonld.count('FAQPage')
            if faq_count > 1:
                issues['duplicate_faqpage'].append({'url': url, 'count': faq_count})

# Find duplicates (titles/descriptions appearing 2+ times)
for title, count in title_counts.items():
    if count > 1 and title:
        matching_urls = [r.get('url', '') or r.get('URL', '') for r in rows 
                        if ((r.get('title', '') or r.get('Title', '')).strip() == title)]
        for url in matching_urls[:5]:  # Limit to first 5
            issues['duplicate_titles'].append({'url': url, 'title': title})

for desc, count in desc_counts.items():
    if count > 1 and desc and len(desc) > 10:
        matching_urls = [r.get('url', '') or r.get('URL', '') for r in rows 
                        if ((r.get('meta_description', '') or r.get('MetaDescription', '')).strip() == desc)]
        for url in matching_urls[:5]:
            issues['duplicate_descriptions'].append({'url': url, 'description': desc})

# Summary
print(f"\nğŸ” Issues Found:")
print(f"  ğŸ”´ {len(issues['empty_titles'])} empty titles")
print(f"  ğŸ”´ {len(issues['empty_descriptions'])} empty descriptions")
print(f"  ğŸŸ¡ {len(issues['duplicate_titles'])} duplicate titles")
print(f"  ğŸŸ¡ {len(issues['duplicate_descriptions'])} duplicate descriptions")
print(f"  ğŸŸ  {len(issues['missing_canonicals'])} missing canonicals")
print(f"  ğŸŸ  {len(issues['http_canonicals'])} http:// canonicals (should be https)")
print(f"  âš ï¸  {len(issues['duplicate_faqpage'])} pages with duplicate FAQPage schema")

# Generate briefs for problem pages
target_urls = set()
for issue_type, issue_list in issues.items():
    for item in issue_list[:50]:  # Limit to top 50 per issue
        target_urls.add(item.get('url', ''))

target_urls = {u for u in target_urls if u and u != 'nan'}

print(f"\nğŸ“ Generating {len(target_urls)} SEO briefs...")

NOW = datetime.datetime.now(datetime.UTC).strftime("%Y-%m-%d")

for url in sorted(target_urls):
    # Extract keywords from URL
    url_tokens = tokens(url)
    guess_kw = " ".join(url_tokens[:4]) if url_tokens else "page"
    
    # Get current meta (if any)
    current = next((r for r in rows if (r.get('url', '') or r.get('URL', '')) == url), {})
    current_title = (current.get('title', '') or current.get('Title', '')).strip()
    current_desc = (current.get('meta_description', '') or current.get('MetaDescription', '')).strip()
    
    # Generate suggestions
    title_suggestion = cap(f"{guess_kw.title()} | {netloc or 'NRLC.ai'}", 60)
    h1_suggestion = cap(guess_kw.title(), 70)
    meta_suggestion = cap(f"{guess_kw} â€” Expert solutions and implementation guide. Updated {NOW}.", 160)
    
    # Subheads based on URL structure
    if '/services/' in url:
        subs = [
            f"Why Choose Our {guess_kw.title()} Service",
            f"How {guess_kw.title()} Works",
            f"{guess_kw.title()} Pricing & Packages"
        ]
    elif '/insights/' in url or '/blog/' in url:
        subs = [
            f"Understanding {guess_kw.title()}",
            f"Best Practices for {guess_kw.title()}",
            f"Common {guess_kw.title()} Challenges"
        ]
    else:
        subs = [
            f"{guess_kw.title()} Overview",
            f"Key Benefits",
            f"Implementation Guide"
        ]
    
    # FAQ schema
    faq_ld = {
        "@context": "https://schema.org",
        "@type": "FAQPage",
        "mainEntity": [
            {
                "@type": "Question",
                "name": f"What is {guess_kw}?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Provide a clear, concise definition and explain the core value proposition."
                }
            },
            {
                "@type": "Question",
                "name": f"How do I implement {guess_kw}?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Outline the step-by-step process, timeline, and expected outcomes."
                }
            },
            {
                "@type": "Question",
                "name": f"What are the benefits of {guess_kw}?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "List the main advantages, ROI potential, and competitive advantages."
                }
            }
        ]
    }
    
    # Identify specific issues for this URL
    url_issues = []
    if any(i.get('url') == url for i in issues['empty_titles']):
        url_issues.append("âŒ Empty title tag")
    if any(i.get('url') == url for i in issues['empty_descriptions']):
        url_issues.append("âŒ Empty meta description")
    if any(i.get('url') == url for i in issues['duplicate_titles']):
        url_issues.append("âš ï¸  Duplicate title (conflicts with other pages)")
    if any(i.get('url') == url for i in issues['duplicate_descriptions']):
        url_issues.append("âš ï¸  Duplicate description (conflicts with other pages)")
    if any(i.get('url') == url for i in issues['missing_canonicals']):
        url_issues.append("âš ï¸  Missing canonical tag")
    if any(i.get('url') == url for i in issues['http_canonicals']):
        url_issues.append("ğŸ”’ Canonical uses http:// (should be https://)")
    if any(i.get('url') == url for i in issues['duplicate_faqpage']):
        url_issues.append("âš ï¸  Duplicate FAQPage schema blocks")
    
    brief = f"""# SEO Fix-Now Brief

**URL:** {https(url)}  
**Generated:** {NOW}  
**Priority:** {'ğŸ”´ CRITICAL' if any('Empty' in i for i in url_issues) else 'ğŸŸ¡ High'}

---

## ğŸš¨ Current Issues

{chr(10).join(url_issues) if url_issues else "No critical issues detected."}

---

## Current State

**Current Title:** {current_title if current_title and current_title != 'nan' else '(empty)'}  
**Current Description:** {current_desc if current_desc and current_desc != 'nan' else '(empty)'}

---

## âœ… Recommended Fixes

### Title Tag (â‰¤60 chars)
```html
<title>{title_suggestion}</title>
```

**Length:** {len(title_suggestion)} chars {'âœ“' if len(title_suggestion) <= 60 else 'âš ï¸  too long'}

---

### H1 Heading
```html
<h1>{h1_suggestion}</h1>
```

---

### Meta Description (~155 chars)
```html
<meta name="description" content="{meta_suggestion}">
```

**Length:** {len(meta_suggestion)} chars {'âœ“' if 150 <= len(meta_suggestion) <= 160 else 'âš ï¸  adjust length'}

---

### Canonical Tag (HTTPS only!)
```html
<link rel="canonical" href="{https(url)}">
```

---

## ğŸ“„ Content Structure

### Intro Paragraph (first 2-3 sentences)
Start with the user's primary intent and expected outcome. Establish authority with proof signals (years in business, certifications, case study results). End with a clear next step.

Example:
> "Looking for {guess_kw}? Our team has delivered {guess_kw} solutions to 200+ clients since 2020, achieving an average 35% improvement in [key metric]. This guide walks you through our proven methodology, from initial assessment to full deployment."

---

### Suggested Subheadings (H2)

1. **{subs[0]}**
   - Bullet: Key advantage 1
   - Bullet: Key advantage 2
   - Bullet: Proof point (testimonial, case study, metric)

2. **{subs[1]}**
   - Step 1: [action]
   - Step 2: [action]
   - Step 3: [action]
   - Timeline: X weeks

3. **{subs[2]}**
   - Pricing overview
   - Package comparison
   - ROI expectations
   - Next steps / CTA

---

## â“ FAQ Section (+ JSON-LD)

Add this FAQ section at the bottom of the page, then include the JSON-LD schema:

<details>
<summary><strong>What is {guess_kw}?</strong></summary>
<p>Provide a clear, concise definition and explain the core value proposition.</p>
</details>

<details>
<summary><strong>How do I implement {guess_kw}?</strong></summary>
<p>Outline the step-by-step process, timeline, and expected outcomes.</p>
</details>

<details>
<summary><strong>What are the benefits of {guess_kw}?</strong></summary>
<p>List the main advantages, ROI potential, and competitive advantages.</p>
</details>

### JSON-LD Schema (add to <head>)

```json
{json.dumps(faq_ld, ensure_ascii=False, indent=2)}
```

**âš ï¸  IMPORTANT:** Use `SchemaFixes::jsonLdOnce()` to prevent duplicate schema blocks!

---

## ğŸ”— Internal Links

Add 2-3 contextual internal links to related pages:

- Related service: `https://{netloc}/services/[related-service]/`
- Related insight: `https://{netloc}/insights/[related-article]/`
- Case study: `https://{netloc}/case-studies/[relevant-case]/`

---

## âœ… Validation Checklist

Before deploying:

- [ ] Title is unique and â‰¤60 characters
- [ ] Meta description is unique and 150-160 characters
- [ ] H1 matches title intent (but not identical)
- [ ] Canonical tag uses `https://` (not `http://`)
- [ ] FAQ JSON-LD is valid (test at schema.org/validator)
- [ ] No duplicate schema blocks (use `SchemaFixes::jsonLdOnce()`)
- [ ] Internal links are contextual and relevant
- [ ] Intro paragraph includes proof signals
- [ ] CTA is clear and actionable

---

## ğŸ“Š Expected Impact

| Metric | Before | Target | Timeline |
|--------|--------|--------|----------|
| Google Index | {'Not indexed' if 'empty' in str(url_issues).lower() else 'Indexed'} | Fully indexed | 1-2 weeks |
| SERP CTR | â€” | +2-5% | 2-4 weeks |
| Avg Position | â€” | +3-8 ranks | 4-8 weeks |
| Conversions | â€” | +10-20% | 8-12 weeks |

---

**Next:** Apply these changes, then validate with:
1. Google Rich Results Test: https://search.google.com/test/rich-results
2. Schema Validator: https://validator.schema.org/
3. Request indexing in GSC: Search Console â†’ URL Inspection â†’ Request Indexing

"""
    
    # Safe filename
    name = re.sub(r"[^a-z0-9]+", "-", urllib.parse.urlparse(url).path.strip("/").lower()) or "home"
    with open(os.path.join(OUTB, f"{name}.md"), "w", encoding="utf-8") as f:
        f.write(brief)

print(f"âœ… Generated {len(target_urls)} briefs")

# Create remediation templates
meta_template = f"""<!-- Meta Tags Template (use in <head>) -->

<!-- Primary Meta Tags -->
<title>{{{{ page_title }}}}</title>
<meta name="description" content="{{{{ page_description }}}}">
<link rel="canonical" href="{{{{ canonical_url }}}}">

<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="{{{{ canonical_url }}}}">
<meta property="og:title" content="{{{{ page_title }}}}">
<meta property="og:description" content="{{{{ page_description }}}}">
<meta property="og:image" content="https://{netloc}/assets/og-image.png">

<!-- Twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:url" content="{{{{ canonical_url }}}}">
<meta name="twitter:title" content="{{{{ page_title }}}}">
<meta name="twitter:description" content="{{{{ page_description }}}}">
<meta name="twitter:image" content="https://{netloc}/assets/og-image.png">

<!-- 
REQUIREMENTS:
- page_title: â‰¤60 characters
- page_description: 150-160 characters
- canonical_url: MUST start with https://
- og:image: 1200x630px minimum
-->
"""

with open(os.path.join(OUTR, "fixes", "meta_template.html"), "w", encoding="utf-8") as f:
    f.write(meta_template)

# PHP helper (already exists but make sure it's available)
schema_helper = """<?php
declare(strict_types=1);
namespace NRLC\\Schema;

/**
 * Schema Normalization Helpers
 * Fixes common GSC schema issues
 */
final class SchemaFixes {
    
    /**
     * Ensure URLs use HTTPS
     */
    public static function ensureHttps(?string $url): ?string {
        if (!$url) return $url;
        if (stripos($url, 'https://') === 0) return $url;
        if (stripos($url, 'http://') === 0) return 'https://' . substr($url, 7);
        return $url;
    }
    
    /**
     * Prevent duplicate JSON-LD blocks with same @id
     * Usage: echo SchemaFixes::jsonLdOnce($schema_array);
     */
    public static function jsonLdOnce($jsonOrArray, string $idKey = '@id'): ?string {
        static $seen = [];
        
        $decoded = is_array($jsonOrArray) 
            ? $jsonOrArray 
            : json_decode((string)$jsonOrArray, true);
        
        if (!is_array($decoded)) return null;
        
        // Handle both single objects and arrays of objects
        $items = (array_keys($decoded) !== range(0, count($decoded) - 1)) 
            ? [$decoded] 
            : $decoded;
        
        foreach ($items as $obj) {
            if (isset($obj[$idKey])) {
                $id = (string)$obj[$idKey];
                if (isset($seen[$id])) {
                    // Already output, skip
                    return null;
                }
                $seen[$id] = true;
            }
        }
        
        return json_encode($decoded, 
            JSON_UNESCAPED_SLASHES | 
            JSON_UNESCAPED_UNICODE | 
            JSON_INVALID_UTF8_SUBSTITUTE
        );
    }
    
    /**
     * Reset seen IDs (for testing)
     */
    public static function resetSeen(): void {
        static $seen = [];
        $seen = [];
    }
}
"""

with open(os.path.join(OUTR, "fixes", "SchemaFixes.php"), "w", encoding="utf-8") as f:
    f.write(schema_helper)

# Checklist
checklist = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          FIX-NOW CHECKLIST (Google Best Practices)            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Generated: {datetime.datetime.now(datetime.UTC).strftime('%Y-%m-%d %H:%M:%S')} UTC
Source: site-discovery audit report ({len(rows)} pages analyzed)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ğŸ”´ CRITICAL (Fix First)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ ] Fix {len(issues['empty_titles'])} empty title tags
    â†’ Without titles, pages won't rank in Google
    â†’ Action: See individual briefs in website/seo-briefs/
    â†’ Validation: View source, check <title> tag is present

[ ] Fix {len(issues['empty_descriptions'])} empty meta descriptions
    â†’ Google will auto-generate (usually poor quality)
    â†’ Action: Write unique 150-160 char descriptions
    â†’ Validation: Search Console â†’ Enhancements â†’ Meta tags

[ ] Fix {len(issues['http_canonicals'])} http:// canonical tags
    â†’ Forces HTTPS normalization issues
    â†’ Action: Change all http:// to https:// in canonicals
    â†’ Validation: grep -r 'rel="canonical" href="http://' pages/

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ğŸŸ¡ HIGH PRIORITY (Do This Week)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ ] Differentiate {len(issues['duplicate_titles'])} duplicate titles
    â†’ Google penalizes identical titles across pages
    â†’ Action: Make each title unique (add location, year, differentiator)
    â†’ Validation: site-discovery or Screaming Frog

[ ] Differentiate {len(issues['duplicate_descriptions'])} duplicate descriptions
    â†’ Reduces CTR and click-through rate
    â†’ Action: Write page-specific descriptions
    â†’ Validation: Search Console â†’ Coverage â†’ Duplicates

[ ] Fix {len(issues['missing_canonicals'])} missing canonical tags
    â†’ Without canonicals, Google may index duplicate pages
    â†’ Action: Add <link rel="canonical" href="https://..." />
    â†’ Validation: curl -s https://nrlc.ai/page/ | grep canonical

[ ] Fix {len(issues['duplicate_faqpage'])} duplicate FAQPage schema
    â†’ Causes GSC "Duplicate field" error
    â†’ Action: Use SchemaFixes::jsonLdOnce() in lib/schema_builders.php
    â†’ Validation: Google Rich Results Test

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ğŸŸ¢ MEDIUM PRIORITY (This Month)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ ] Add FAQ sections with JSON-LD to top 50 pages
    â†’ Increases SERP real estate (rich results)
    â†’ Action: Use templates from website/seo-briefs/
    â†’ Validation: https://search.google.com/test/rich-results

[ ] Expand intro paragraphs with E-E-A-T signals
    â†’ Proof points: years in business, certifications, results
    â†’ Action: See "Intro Paragraph" section in briefs
    â†’ Validation: Manual review

[ ] Add 2-3 internal links per page
    â†’ Improves crawlability and PageRank distribution
    â†’ Action: Link to related Services, Insights, Case Studies
    â†’ Validation: Check internal link count in audit_report.csv

[ ] Optimize title lengths (check manually)
    â†’ Google truncates at ~60 chars
    â†’ Action: Cap at 60, move secondary keywords to description
    â†’ Validation: SERP preview tool

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ğŸ“‹ VALIDATION STEPS (After Deployment)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. [ ] Re-run site-discovery crawler
   â†’ php site-discovery/scripts/discover.php --base=https://nrlc.ai --max=500
   â†’ Confirm issues are resolved in new audit_report.csv

2. [ ] Test schema in Google Rich Results Test
   â†’ https://search.google.com/test/rich-results
   â†’ Test 5-10 pages with FAQ/schema changes

3. [ ] Validate canonicals
   â†’ curl -s https://nrlc.ai | grep -i canonical
   â†’ Confirm all use https://

4. [ ] Request re-indexing in Search Console
   â†’ URL Inspection â†’ Request Indexing (for critical pages)
   â†’ Or submit updated sitemap

5. [ ] Monitor GSC Enhancements
   â†’ Check "Enhancements" section after 1-2 weeks
   â†’ Confirm "Duplicate field" errors are gone

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 ğŸ“š RESOURCES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

- Google Search Essentials: https://developers.google.com/search/docs/essentials
- Schema.org Validator: https://validator.schema.org/
- Rich Results Test: https://search.google.com/test/rich-results
- Page Speed Insights: https://pagespeed.web.dev/
- GSC URL Inspection: https://search.google.com/search-console

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Next: Open website/seo-briefs/ and start with ğŸ”´ CRITICAL pages
"""

with open(os.path.join(OUTR, "fixes", "checklist.txt"), "w", encoding="utf-8") as f:
    f.write(checklist)

# Summary report
summary = f"""# Fix-Now Analysis Summary

**Generated:** {datetime.datetime.now(datetime.UTC).strftime('%Y-%m-%d %H:%M:%S')} UTC  
**Source:** site-discovery audit report  
**Pages Analyzed:** {len(rows)}  
**Pages Needing Fixes:** {len(target_urls)}

---

## ğŸš¨ Issues Detected

### Critical (Fix First)

| Issue | Count | Impact | Priority |
|-------|-------|--------|----------|
| Empty Titles | **{len(issues['empty_titles'])}** | Won't rank in Google | ğŸ”´ CRITICAL |
| Empty Descriptions | **{len(issues['empty_descriptions'])}** | Poor SERP snippets | ğŸ”´ CRITICAL |
| HTTP Canonicals | **{len(issues['http_canonicals'])}** | HTTPS normalization issues | ğŸ”´ CRITICAL |

### High Priority

| Issue | Count | Impact | Priority |
|-------|-------|--------|----------|
| Duplicate Titles | **{len(issues['duplicate_titles'])}** | Confuses Google, reduces rankings | ğŸŸ¡ High |
| Duplicate Descriptions | **{len(issues['duplicate_descriptions'])}** | Lower CTR | ğŸŸ¡ High |
| Missing Canonicals | **{len(issues['missing_canonicals'])}** | Duplicate content risk | ğŸŸ¡ High |
| Duplicate FAQPage | **{len(issues['duplicate_faqpage'])}** | GSC enhancement error | ğŸŸ¡ High |

---

## ğŸ“Š Generated Outputs

### 1. SEO Briefs (`website/seo-briefs/`)
- **Count:** {len(target_urls)} page-level briefs
- **Contents:** Title, H1, meta description, FAQs, internal links, validation checklist
- **Format:** Markdown (ready to apply)

### 2. Remediation Kit (`website/seo-remediation/fixes/`)
- `meta_template.html` - Optimal <head> structure with Open Graph & Twitter cards
- `SchemaFixes.php` - HTTPS normalization and JSON-LD deduplication helpers
- `checklist.txt` - Developer action items with validation steps

### 3. This Summary
- Issue breakdown
- Expected impact timeline
- Quick start guide

---

## ğŸ¯ Recommended Action Plan

### Week 1: Fix Critical Issues
1. âœ… Apply fixes to all pages with empty titles ({len(issues['empty_titles'])} pages)
2. âœ… Apply fixes to all pages with empty descriptions ({len(issues['empty_descriptions'])} pages)
3. âœ… Change all `http://` canonicals to `https://` ({len(issues['http_canonicals'])} pages)

**Tools:**
- Briefs: `website/seo-briefs/`
- Template: `website/seo-remediation/fixes/meta_template.html`

**Validation:**
```bash
# Check for empty titles
php scripts/comprehensive_page_scan.php | grep "empty title"

# Check for http:// canonicals
grep -r 'rel="canonical" href="http://' pages/
```

### Week 2: Differentiate Duplicates
1. âœ… Make duplicate titles unique (add location, year, or differentiator)
2. âœ… Rewrite duplicate descriptions (page-specific value props)
3. âœ… Add missing canonical tags

**Expected Result:** Each page has unique title/description

### Week 3: Enhance Rich Results
1. âœ… Add FAQ sections with JSON-LD to top 50 pages
2. âœ… Use `SchemaFixes::jsonLdOnce()` to prevent duplicate schema
3. âœ… Test with Google Rich Results Test

**Expected Result:** FAQ rich results in SERP

### Week 4: Validate & Monitor
1. âœ… Re-run site-discovery crawler (confirm fixes)
2. âœ… Request re-indexing in Google Search Console
3. âœ… Monitor GSC Enhancements tab for errors

**Expected Result:** All GSC enhancement errors resolved

---

## ğŸ“ˆ Expected Impact

| Timeframe | Metric | Change | Reason |
|-----------|--------|--------|--------|
| **Week 1** | Indexation | +100% (for empty title pages) | Google can now understand pages |
| **Week 2** | SERP CTR | +2-5% | Unique, compelling meta descriptions |
| **Week 3-4** | Avg Position | +3-8 ranks | Better relevance signals |
| **Month 2** | Organic Traffic | +20-40% | Combined effect of fixes |
| **Month 3** | Rich Results | 50+ pages eligible | FAQ structured data |

---

## ğŸ› ï¸ Tools & Integration

### Use SchemaFixes.php in Your Code

```php
<?php
require_once __DIR__.'/lib/SchemaFixes.php';
use NRLC\\Schema\\SchemaFixes;

// Force HTTPS on all schema URLs
$schema = [
    '@type' => 'Organization',
    'url' => SchemaFixes::ensureHttps($org_url),
    'logo' => SchemaFixes::ensureHttps($logo_url)
];

// Prevent duplicate schema blocks
$jsonLd = SchemaFixes::jsonLdOnce($schema);
if ($jsonLd) {{
    echo '<script type="application/ld+json">' . $jsonLd . '</script>';
}}
```

### Already Integrated
The `SchemaFixes` utility has been added to `/lib/SchemaFixes.php` and integrated into `lib/schema_builders.php` to automatically enforce HTTPS on all schema URLs.

---

## âœ… Quick Start

1. **Read a brief:** Open `website/seo-briefs/[page-name].md`
2. **Apply the fix:** Update title, H1, meta description, add FAQ
3. **Use the template:** Copy from `website/seo-remediation/fixes/meta_template.html`
4. **Validate:** Google Rich Results Test + GSC URL Inspection
5. **Deploy & monitor:** Check GSC after 1-2 weeks

---

## ğŸ”„ Re-Run Analysis

After applying fixes, re-run to verify:

```bash
# Re-crawl your site
php site-discovery/scripts/discover.php --base=https://nrlc.ai --max=500

# Re-run analysis
python3 tools/fixnow_from_site_discovery.py

# Compare new vs old FIXNOW_SUMMARY.md
```

---

## ğŸ“š References

- [Google Search Essentials](https://developers.google.com/search/docs/essentials)
- [Meta Tags Best Practices](https://developers.google.com/search/docs/appearance/snippet)
- [Schema.org Validator](https://validator.schema.org/)
- [Google Rich Results Test](https://search.google.com/test/rich-results)

---

**Next Step:** Open `website/seo-briefs/` and start with the ğŸ”´ CRITICAL pages!
"""

with open(os.path.join(OUTR, "FIXNOW_SUMMARY.md"), "w", encoding="utf-8") as f:
    f.write(summary)

print(f"\n{'='*65}")
print("âœ… Fix-Now Analysis Complete!")
print(f"{'='*65}")
print(f"\nPriorities:")
print(f"  ğŸ”´ {len(issues['empty_titles'])} empty titles â†’ FIX FIRST")
print(f"  ğŸ”´ {len(issues['empty_descriptions'])} empty descriptions â†’ FIX FIRST")
print(f"  ğŸ”´ {len(issues['http_canonicals'])} http:// canonicals â†’ FIX FIRST")
print(f"  ğŸŸ¡ {len(issues['duplicate_titles'])} duplicate titles")
print(f"  ğŸŸ¡ {len(issues['duplicate_descriptions'])} duplicate descriptions")
print(f"  âš ï¸  {len(issues['duplicate_faqpage'])} duplicate FAQPage schema")
print(f"\nOutputs:")
print(f"  ğŸ“ Briefs â†’ {OUTB}")
print(f"  ğŸ”§ Templates â†’ {OUTR}/fixes/")
print(f"  ğŸ“Š Summary â†’ {OUTR}/FIXNOW_SUMMARY.md")
print(f"\nğŸ‘‰ Next: open {OUTR}/FIXNOW_SUMMARY.md")
print(f"{'='*65}\n")

