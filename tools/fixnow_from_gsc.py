import os, re, csv, io, json, datetime, urllib.parse
import pandas as pd
from collections import defaultdict

# Use project-relative paths
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GSC = os.path.join(PROJECT_ROOT, "gsc_data")
OUTB = os.path.join(PROJECT_ROOT, "website", "seo-briefs")
OUTR = os.path.join(PROJECT_ROOT, "website", "seo-remediation")
os.makedirs(OUTB, exist_ok=True)
os.makedirs(os.path.join(OUTR, "fixes"), exist_ok=True)

print(f"Reading GSC data from: {GSC}")

def read_csv_any(path):
    if not os.path.exists(path):
        print(f"  Warning: {os.path.basename(path)} not found, skipping")
        return None
    for enc in ("utf-8", "utf-8-sig", "latin-1"):
        try:
            df = pd.read_csv(path, encoding=enc)
            print(f"  ‚úì Loaded {os.path.basename(path)}: {len(df)} rows")
            return df
        except Exception as e:
            continue
    try:
        df = pd.read_csv(path, engine="python")
        print(f"  ‚úì Loaded {os.path.basename(path)}: {len(df)} rows")
        return df
    except Exception as e:
        print(f"  ‚úó Failed to read {os.path.basename(path)}: {e}")
        return None

def pick_col(df, candidates):
    if df is None or df.empty:
        return None
    for c in df.columns:
        cl = c.lower().strip()
        if cl in candidates:
            return c
    for c in df.columns:
        cl = c.lower().strip()
        for k in candidates:
            if k in cl:
                return c
    return df.columns[0] if len(df.columns) > 0 else None

def cap(s, n):
    s = re.sub(r"\s+", " ", str(s)).strip()
    return (s[:n-1]+"‚Ä¶") if len(s) > n else s

def https(u):
    if not isinstance(u, str):
        return u
    if u.startswith("https://"):
        return u
    if u.startswith("http://"):
        return "https://" + u[7:]
    return u

# Load core data
tdf = read_csv_any(os.path.join(GSC, "Table.csv"))
mdf = read_csv_any(os.path.join(GSC, "Metadata.csv"))

if tdf is None and mdf is None:
    print("\n‚ùå Error: Need at least Table.csv or Metadata.csv")
    exit(1)

# Optional: Load Queries and Pages for position/impression data
qdf = read_csv_any(os.path.join(GSC, "Queries.csv"))
pdf = read_csv_any(os.path.join(GSC, "Pages.csv"))

# URL columns
if tdf is not None:
    t_url = pick_col(tdf, {"url", "page", "page url", "canonical", "link", "address", "location", "top pages"})
    if t_url:
        tdf["__url"] = tdf[t_url].astype(str)
    else:
        tdf["__url"] = ""

if mdf is not None:
    m_url = pick_col(mdf, {"url", "page", "page url", "address", "location", "canonical", "top pages"})
    if m_url:
        mdf["__url"] = mdf[m_url].astype(str)
    else:
        mdf["__url"] = ""

# Position/impressions (try Table first, then Pages)
pos_col = None
imp_col = None
pos_df = tdf if tdf is not None else pdf

if pos_df is not None:
    for c in pos_df.columns:
        cl = c.lower().strip()
        if cl in ("position", "avg position", "average position"):
            pos_col = c
        if cl in ("impressions", "impr", "impression"):
            imp_col = c

# Striking-distance pages (position 5-15)
strike = []
if pos_df is not None and pos_col is not None and "__url" in pos_df.columns:
    pos_series = pd.to_numeric(pos_df[pos_col], errors="coerce")
    strike_df = pos_df[pos_series.between(5, 15, inclusive="both")]
    if not strike_df.empty:
        strike = strike_df[["__url", pos_col]].sort_values(pos_col).head(100).to_dict("records")
        print(f"\nüìä Found {len(strike)} striking-distance pages (position 5-15)")

# Metadata issues
title_col = None
desc_col = None
if mdf is not None:
    for c in mdf.columns:
        cl = c.lower().strip()
        if cl in ("title", "page title", "meta title"):
            title_col = c
        if cl in ("description", "meta description", "meta desc"):
            desc_col = c
    if title_col is None:
        for c in mdf.columns:
            if "title" in c.lower():
                title_col = c
                break
    if desc_col is None:
        for c in mdf.columns:
            if "description" in c.lower() or "meta" in c.lower():
                desc_col = c
                break

dups = {"duplicate_titles": [], "duplicate_metas": [], "empty_titles": [], "empty_metas": []}

if mdf is not None and "__url" in mdf.columns:
    if title_col:
        vt = mdf[title_col].astype(str).str.strip()
        dup_t_vals = vt.value_counts()
        dup_t_vals = dup_t_vals[dup_t_vals > 1].index.tolist()
        dups["duplicate_titles"] = mdf[mdf[title_col].astype(str).str.strip().isin(dup_t_vals)][["__url", title_col]].head(300).to_dict("records")
        dups["empty_titles"] = mdf[vt.eq("")][["__url", title_col]].head(300).to_dict("records")
        print(f"  ‚ö†Ô∏è  {len(dups['duplicate_titles'])} duplicate titles")
        print(f"  ‚ö†Ô∏è  {len(dups['empty_titles'])} empty titles")
    
    if desc_col:
        vd = mdf[desc_col].astype(str).str.strip()
        dup_d_vals = vd.value_counts()
        dup_d_vals = dup_d_vals[(dup_d_vals > 1) & (dup_d_vals.index.str.len() > 0)].index.tolist()
        dups["duplicate_metas"] = mdf[mdf[desc_col].astype(str).str.strip().isin(dup_d_vals)][["__url", desc_col]].head(300).to_dict("records")
        dups["empty_metas"] = mdf[vd.eq("")][["__url", desc_col]].head(300).to_dict("records")
        print(f"  ‚ö†Ô∏è  {len(dups['duplicate_metas'])} duplicate descriptions")
        print(f"  ‚ö†Ô∏è  {len(dups['empty_metas'])} empty descriptions")

def tokens(url):
    path = urllib.parse.urlparse(url).path.lower()
    return set([t for t in re.split(r"[^a-z0-9]+", path) if t])

# Make briefs for striking pages + any with meta issues
target_urls = set([r["__url"] for r in strike if "__url" in r])
target_urls.update([r["__url"] for r in dups["empty_titles"]])
target_urls.update([r["__url"] for r in dups["empty_metas"]])
# Add a sample from duplicates
target_urls.update([r["__url"] for r in dups["duplicate_titles"][:20]])
target_urls.update([r["__url"] for r in dups["duplicate_metas"][:20]])

# Remove empty URLs
target_urls = {u for u in target_urls if u and str(u).strip() and str(u) != 'nan'}

print(f"\nüìù Generating briefs for {len(target_urls)} target pages...")

NOW = datetime.datetime.utcnow().strftime("%Y-%m-%d")
netloc = None

for u in sorted(target_urls):
    loc_netloc = urllib.parse.urlparse(u).netloc
    if loc_netloc:
        netloc = loc_netloc
    
    # Extract meaningful keywords from URL
    url_tokens = tokens(u)
    # Remove common words
    url_tokens = {t for t in url_tokens if t not in {'en', 'us', 'gb', 'de', 'fr', 'es', 'ko', 'kr', 'www'}}
    guess_kw = " ".join(sorted(list(url_tokens))[:4]) or "Update"
    
    title = cap(f"{guess_kw} | {loc_netloc or ''}", 60)
    h1 = cap(guess_kw.title(), 70)
    meta = cap(f"{guess_kw} ‚Äî solutions, steps, and FAQs. Updated {NOW}.", 160)
    
    # Subheads
    subs = [f"{guess_kw} benefits", f"{guess_kw} implementation", f"{guess_kw} pricing and ROI"]
    
    faq_ld = {
        "@context": "https://schema.org",
        "@type": "FAQPage",
        "mainEntity": [
            {
                "@type": "Question",
                "name": f"What is {guess_kw}?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Explain the concept, prerequisites, and where it applies."
                }
            },
            {
                "@type": "Question",
                "name": f"How to implement {guess_kw}?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Show steps, timeline, and responsibilities."
                }
            }
        ]
    }
    
    brief = f"""# Page SEO Brief
URL: {https(u)}

## Title (<=60)
{title}

## H1
{h1}

## Meta Description (150‚Äì160)
{meta}

## Intro Paragraph
Open with the user's intent and expected outcome in 2‚Äì3 sentences. Mention proof signals and next step.

## Suggested Subheads
- {subs[0]}
- {subs[1]}
- {subs[2]}

## Internal Links
- https://{netloc or 'example.com'}/services/
- https://{netloc or 'example.com'}/insights/

## HTTPS Normalization
- Canonical and schema URLs should be https

## FAQ (and JSON-LD)
```json
{json.dumps(faq_ld, ensure_ascii=False, indent=2)}
```
"""
    name = re.sub(r"[^a-z0-9]+", "-", urllib.parse.urlparse(u).path.strip("/").lower()) or "home"
    with open(os.path.join(OUTB, f"{name}.md"), "w", encoding="utf-8") as f:
        f.write(brief)

print(f"‚úÖ Generated {len(target_urls)} SEO briefs")

# Remediation kit
meta_tpl = """<!-- Head template example -->
<title>{{ dynamic_title }}</title>
<meta name="description" content="{{ dynamic_description }}">
<link rel="canonical" href="{{ canonical_url }}">
<meta property="og:type" content="website">
<meta property="og:title" content="{{ dynamic_title }}">
<meta property="og:description" content="{{ dynamic_description }}">
<meta property="og:url" content="{{ canonical_url }}">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="{{ dynamic_title }}">
<meta name="twitter:description" content="{{ dynamic_description }}">
"""
with open(os.path.join(OUTR, "fixes", "meta_template.html"), "w", encoding="utf-8") as f:
    f.write(meta_tpl)

schema_php = """<?php
declare(strict_types=1);
namespace NRLC\\Schema;

final class SchemaFixes {
    public static function ensureHttps(?string $url): ?string {
        if (!$url) return $url;
        if (stripos($url, 'https://')===0) return $url;
        if (stripos($url, 'http://')===0) return 'https://'.substr($url, 7);
        return $url;
    }
    
    public static function jsonLdOnce($jsonOrArray, string $idKey='@id'): ?string {
        static $seen=[];
        $decoded = is_array($jsonOrArray) ? $jsonOrArray : json_decode((string)$jsonOrArray, true);
        if (!is_array($decoded)) return null;
        $items = (array_keys($decoded) !== range(0, count($decoded)-1)) ? [$decoded] : $decoded;
        foreach ($items as $obj) {
            if (isset($obj[$idKey])) {
                $id = (string)$obj[$idKey];
                if (isset($seen[$id])) return null;
                $seen[$id] = true;
            }
        }
        return json_encode($decoded, JSON_UNESCAPED_SLASHES|JSON_UNESCAPED_UNICODE|JSON_INVALID_UTF8_SUBSTITUTE);
    }
}
"""
with open(os.path.join(OUTR, "fixes", "schema_fixes.php"), "w", encoding="utf-8") as f:
    f.write(schema_php)

checklist = f"""Fix-now Checklist (Google best practices)

=== IMMEDIATE ACTIONS ===

META TAGS
[ ] Replace any http:// with https:// in canonicals and schema
[ ] Unique titles (<= 60 chars) per page
[ ] Unique meta descriptions (~155 chars) per page
[ ] Fix {len(dups['empty_titles'])} empty titles
[ ] Fix {len(dups['empty_metas'])} empty descriptions
[ ] Differentiate {len(dups['duplicate_titles'])} duplicate titles
[ ] Differentiate {len(dups['duplicate_metas'])} duplicate descriptions

CONTENT
[ ] Add FAQ section if you include FAQPage JSON-LD
[ ] Add internal links to relevant Services/Insights
[ ] Expand intro to 2-3 sentences (intent + outcome)
[ ] Use suggested subheads from briefs

TECHNICAL
[ ] One canonical per page, https only
[ ] Ensure all schema URLs use https://
[ ] Add Open Graph and Twitter meta tags

=== STRIKING DISTANCE PAGES ({len(strike)}) ===
These pages are position 5-15 and need immediate optimization:

"""

for i, page in enumerate(strike[:20], 1):
    checklist += f"{i}. {page.get('__url', 'N/A')} (pos: {page.get(pos_col if pos_col else 'position', 'N/A')})\n"

if len(strike) > 20:
    checklist += f"... and {len(strike) - 20} more\n"

checklist += f"""
=== VALIDATION ===
[ ] Test in Google Rich Results Test
[ ] Verify canonical tags in HTML source
[ ] Check meta tags length (Chrome DevTools)
[ ] Revalidate in Search Console after deployment

Generated: {datetime.datetime.utcnow().isoformat()}Z
Source: {len(target_urls)} URLs analyzed from GSC data
"""

with open(os.path.join(OUTR, "fixes", "checklist.txt"), "w", encoding="utf-8") as f:
    f.write(checklist)

# Summary report
summary = f"""# Fix-Now GSC Analysis Summary

**Generated:** {datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC

## Data Sources
- Table.csv: {'‚úì' if tdf is not None else '‚úó'}
- Metadata.csv: {'‚úì' if mdf is not None else '‚úó'}
- Pages.csv: {'‚úì' if pdf is not None else '‚úó'} (optional)
- Queries.csv: {'‚úì' if qdf is not None else '‚úó'} (optional)

## Issues Found

### üéØ Striking Distance Pages
**Count:** {len(strike)}  
**Status:** Position 5-15 (close to page 1!)  
**Action:** Optimize title, meta, add FAQs ‚Üí move to positions 1-5

### ‚ö†Ô∏è Meta Tag Issues

| Issue | Count | Priority |
|-------|-------|----------|
| Empty Titles | {len(dups['empty_titles'])} | üî¥ CRITICAL |
| Empty Descriptions | {len(dups['empty_metas'])} | üî¥ CRITICAL |
| Duplicate Titles | {len(dups['duplicate_titles'])} | üü° High |
| Duplicate Descriptions | {len(dups['duplicate_metas'])} | üü° High |

## Generated Outputs

### SEO Briefs
**Location:** `website/seo-briefs/`  
**Count:** {len(target_urls)} page-level briefs  
**Contents:** Title, H1, meta, subheads, FAQs with JSON-LD

### Remediation Kit
**Location:** `website/seo-remediation/fixes/`  
**Files:**
- `meta_template.html` - Optimal <head> structure
- `schema_fixes.php` - HTTPS normalization helpers
- `checklist.txt` - Developer action items

## Quick Start

1. **Review briefs:** Open `website/seo-briefs/` and read page recommendations
2. **Priority 1:** Fix empty titles and descriptions (CRITICAL)
3. **Priority 2:** Optimize striking-distance pages (high ROI)
4. **Priority 3:** Differentiate duplicate meta tags
5. **Deploy & Validate:** Use checklist.txt for validation steps

## Expected Impact

| Timeframe | Metric | Expected Change |
|-----------|--------|-----------------|
| Week 1 | Empty meta fixed | Indexation improved |
| Week 2 | Striking pages optimized | Position +2-5 ranks |
| Week 3-4 | Duplicate meta differentiated | CTR +2-5% |
| Month 2 | Combined effect | Traffic +20-40% |

## Tools Used
- pandas for CSV analysis
- Google Search Essentials guidelines
- Schema.org validation rules

---

*Run `python3 tools/fixnow_from_gsc.py` monthly to regenerate with fresh GSC data*
"""

with open(os.path.join(OUTR, "FIXNOW_SUMMARY.md"), "w", encoding="utf-8") as f:
    f.write(summary)

print(f"\n{'='*60}")
print("‚úÖ Fix-Now GSC Analysis Complete!")
print(f"{'='*60}")
print(f"\nOutputs:")
print(f"  üìù {len(target_urls)} SEO briefs ‚Üí {OUTB}")
print(f"  üîß Remediation kit ‚Üí {OUTR}/fixes/")
print(f"  üìä Summary ‚Üí {OUTR}/FIXNOW_SUMMARY.md")
print(f"\nPriorities:")
print(f"  üî¥ {len(dups['empty_titles'])} empty titles (fix first!)")
print(f"  üî¥ {len(dups['empty_metas'])} empty descriptions (fix first!)")
print(f"  üéØ {len(strike)} striking-distance pages (optimize for quick wins)")
print(f"  üü° {len(dups['duplicate_titles'])} duplicate titles (differentiate)")
print(f"  üü° {len(dups['duplicate_metas'])} duplicate descriptions (differentiate)")
print(f"\nNext: open {OUTR}/FIXNOW_SUMMARY.md")

